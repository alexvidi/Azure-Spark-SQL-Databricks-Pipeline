# Azure-Spark-SQL-Databricks-Pipeline

Este proyecto implementa un pipeline completo de procesamiento de datos utilizando Azure Synapse Analytics, Azure Data Lake Storage, Apache Spark en Databricks y consultas SQL para analizar los datos procesados.

## ğŸ“Œ Estructura del Proyecto

```bash
Azure-Spark-SQL-Databricks-Pipeline/
â”‚â”€â”€ config/
â”‚   â”œâ”€â”€ credentials.yaml  # Archivo de credenciales para conexiones
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ jsonplaceholder_posts.json  # Archivo JSON original con los datos
â”‚   â”œâ”€â”€ jsonplaceholder_posts.csv  # Archivo convertido a CSV para Synapse
â”‚   â”œâ”€â”€ SQL queries_Azure Synapse Analytics.csv  # Resultado de las consultas SQL
â”‚â”€â”€ docs/
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ Spark-SQL 04-03.ipynb  # Notebook con consultas y procesamiento en Databricks
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ azure_connection.py  # Script de conexiÃ³n con Azure
â”‚   â”‚   â”œâ”€â”€ convert_json_to_csv.py  # ConversiÃ³n de JSON a CSV
â”‚   â”‚   â”œâ”€â”€ ingest_jsonplaceholder_data.py  # Ingesta de datos
â”‚â”€â”€ test/
â”‚â”€â”€ utils/
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt  # Dependencias del proyecto
```

---

## ğŸš€ Paso a Paso del Proyecto

### 1ï¸âƒ£ ConfiguraciÃ³n y ConexiÃ³n con Azure
- Se creÃ³ un **Azure Data Lake Storage Gen2** para almacenar los archivos de datos.
- Se configurÃ³ una **cuenta de almacenamiento** en Azure.
- Se estableciÃ³ la conexiÃ³n con **Azure Synapse Analytics** para realizar consultas SQL sobre los datos.
- Se utilizÃ³ el script `azure_connection.py` para gestionar la autenticaciÃ³n y conexiÃ³n con los servicios de Azure.

### 2ï¸âƒ£ Ingesta y ConversiÃ³n de Datos
- Inicialmente, se obtuvo un archivo **JSON** (`jsonplaceholder_posts.json`).
- Se creÃ³ el script `convert_json_to_csv.py` para convertir el JSON en un archivo CSV adecuado para la consulta en Azure Synapse Analytics.
- Se utilizÃ³ la librerÃ­a `pandas` para manipular y guardar los datos en formato CSV.

### 3ï¸âƒ£ Carga de Datos en Azure Data Lake
- Se subiÃ³ el archivo `jsonplaceholder_posts.csv` a **Azure Data Lake Storage**.
- Se generÃ³ una **URL SAS Token** para permitir la consulta desde Azure Synapse.
- Se validÃ³ el acceso a los archivos desde Azure Synapse.

### 4ï¸âƒ£ Procesamiento con Apache Spark en Databricks
- Se utilizÃ³ un **notebook en Databricks** (`Spark-SQL 04-03.ipynb`) para explorar los datos en Apache Spark.
- Se ejecutaron consultas SQL sobre los datos transformados.
- Se verificÃ³ la estructura de los datos en Databricks.

### 5ï¸âƒ£ Consultas en Azure Synapse Analytics
- Se utilizÃ³ `OPENROWSET` en **Azure Synapse SQL** para leer el archivo CSV directamente desde Data Lake.
- Se realizaron consultas SQL para filtrar y analizar los datos.
- Se exportÃ³ el resultado en `SQL queries_Azure Synapse Analytics.csv` para su posterior anÃ¡lisis.

### 6ï¸âƒ£ ExportaciÃ³n y Descarga de Resultados
- Se descargÃ³ el resultado de las consultas SQL en un archivo CSV.
- Se validÃ³ la integridad de los datos despuÃ©s de la consulta.

---

## ğŸ”§ Scripts Importantes

### **ğŸ”¹ ConexiÃ³n a Azure (`azure_connection.py`)
```python
from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient

# ConexiÃ³n con Azure Storage
account_url = "https://datalakealexvidal.dfs.core.windows.net/"
credential = DefaultAzureCredential()
blob_service_client = BlobServiceClient(account_url, credential)
```

### **ğŸ”¹ ConversiÃ³n de JSON a CSV (`convert_json_to_csv.py`)
```python
import pandas as pd
import json
import os

# Definir rutas de entrada y salida
input_path = "C:/Users/alexv/Azure-Spark-SQL-Databricks-Pipeline/data/jsonplaceholder_posts.json"
output_path = "C:/Users/alexv/Azure-Spark-SQL-Databricks-Pipeline/data/jsonplaceholder_posts.csv"

# Cargar JSON y convertir a DataFrame
def convert_json_to_csv(input_path, output_path):
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False, encoding="utf-8")

convert_json_to_csv(input_path, output_path)
```

### **ğŸ”¹ Consulta SQL en Azure Synapse**
```sql
SELECT *
FROM OPENROWSET(
    BULK 'https://datalakealexvidal.dfs.core.windows.net/raw-data-1/jsonplaceholder_posts.csv',
    FORMAT = 'CSV',
    PARSER_VERSION = '2.0'
) AS posts;
```

---

## ğŸ“Œ TecnologÃ­as Utilizadas

- **Azure Synapse Analytics** â†’ Consultas SQL sobre Data Lake
- **Azure Data Lake Storage Gen2** â†’ Almacenamiento de datos
- **Apache Spark en Databricks** â†’ Procesamiento de datos
- **Python (pandas, json)** â†’ ManipulaciÃ³n y conversiÃ³n de datos
- **SQL (T-SQL, Synapse SQL)** â†’ Consultas y anÃ¡lisis de datos
- **Azure Blob Storage** â†’ GestiÃ³n de archivos en la nube

---

## ğŸ“Œ PrÃ³ximos Pasos
- **Automatizar** la carga de datos en Azure usando **Azure Data Factory**.
- **Implementar un pipeline en Apache Airflow** para la orquestaciÃ³n de tareas.
- **Visualizar datos con Power BI** desde Azure Synapse Analytics.

---

## ğŸš€ Contribuciones y Mejora
Si deseas contribuir a este proyecto, puedes hacer un **fork** y enviar un **pull request** con mejoras en la estructura del cÃ³digo o documentaciÃ³n.

Para cualquier duda, Â¡contÃ¡ctame! ğŸ˜Š



